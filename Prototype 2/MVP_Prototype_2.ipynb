{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "M3lrprF1Ebt-",
        "outputId": "962b8fb7-7e6c-422a-dc83-2cba019aab5b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'aapl_historical_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0567045e3ac4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load dataset (replace with actual data fetching logic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0maapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aapl_historical_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example, replace with API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 1. Data Cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aapl_historical_data.csv'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import yfinance as yf\n",
        "\n",
        "# Load dataset\n",
        "aapl = yf.download(\"AAPL\", start=\"2023-01-01\", end=\"2024-03-15\")\n",
        "\n",
        "# Check for duplicate data\n",
        "duplicates = aapl[aapl.duplicated()]\n",
        "print(f\"Number of duplicate rows: {duplicates.shape[0]}\")\n",
        "aapl = aapl.drop_duplicates()\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = aapl.isna().sum()\n",
        "print(\"Missing values per column:\\n\", missing_values)\n",
        "aapl = aapl.dropna()\n",
        "\n",
        "# Check for data overload\n",
        "print(f\"Dataset contains {aapl.shape[0]} rows and {aapl.shape[1]} columns\")\n",
        "if aapl.shape[0] > 100000:\n",
        "    aapl = aapl.sample(10000)\n",
        "\n",
        "# Check for missing trading days\n",
        "full_range = pd.date_range(start=aapl.index.min(), end=aapl.index.max(), freq=\"B\")\n",
        "missing_dates = full_range.difference(aapl.index)\n",
        "print(\"Missing trading days:\\n\", missing_dates)\n",
        "\n",
        "\n",
        "# 2. Compute Descriptive Statistics\n",
        "summary_stats = aapl.describe().T\n",
        "summary_stats[\"skewness\"] = aapl.skew()\n",
        "summary_stats[\"kurtosis\"] = aapl.kurtosis()\n",
        "\n",
        "# 3. PCA (Run only if dimensions exceed threshold)\n",
        "dimension_threshold = 5  # Example threshold\n",
        "num_features = aapl.shape[1]\n",
        "\n",
        "pca_summary = None  # Default if PCA isn't needed\n",
        "if num_features > dimension_threshold:\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(aapl.dropna())\n",
        "\n",
        "    pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
        "    principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "    pca_summary = {\n",
        "        \"explained_variance_ratio\": pca.explained_variance_ratio_.tolist(),\n",
        "        \"components\": pca.components_.tolist(),\n",
        "    }\n",
        "\n",
        "# 4. Bootstrapping\n",
        "def bootstrap_means(data, num_samples=1000):\n",
        "    boot_means = [np.mean(np.random.choice(data, size=len(data), replace=True)) for _ in range(num_samples)]\n",
        "    return np.mean(boot_means), np.std(boot_means)\n",
        "\n",
        "boot_results = {col: bootstrap_means(aapl[col].dropna()) for col in aapl.columns}\n",
        "print(boot_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU2xL6GxJiID",
        "outputId": "dbcfff0f-9f72-41b6-a79e-35e551e8d9ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n",
            "Missing values per column:\n",
            " Price   Ticker\n",
            "Close   AAPL      0\n",
            "High    AAPL      0\n",
            "Low     AAPL      0\n",
            "Open    AAPL      0\n",
            "Volume  AAPL      0\n",
            "dtype: int64\n",
            "Dataset contains 301 rows and 5 columns\n",
            "Missing trading days:\n",
            " DatetimeIndex(['2023-01-16', '2023-02-20', '2023-04-07', '2023-05-29',\n",
            "               '2023-06-19', '2023-07-04', '2023-09-04', '2023-11-23',\n",
            "               '2023-12-25', '2024-01-01', '2024-01-15', '2024-02-19'],\n",
            "              dtype='datetime64[ns]', freq=None)\n",
            "{('Close', 'AAPL'): (np.float64(173.1048673951919), np.float64(0.9609155207254457)), ('High', 'AAPL'): (np.float64(174.3546251106806), np.float64(0.9724479459553281)), ('Low', 'AAPL'): (np.float64(171.4887370026786), np.float64(0.9606015730615576)), ('Open', 'AAPL'): (np.float64(172.76781896252575), np.float64(0.9735209754551845)), ('Volume', 'AAPL'): (np.float64(59421807.14983388), np.float64(1038936.5932042039))}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Store Statistical Summary in AWS S3\n",
        "statistical_memory = {\n",
        "    \"summary_stats\": summary_stats.to_dict(),\n",
        "    \"pca_summary\": pca_summary,\n",
        "    \"bootstrap_estimates\": boot_results,\n",
        "}\n",
        "\n",
        "compressed_data = json.dumps(statistical_memory)\n",
        "\n",
        "# Upload to AWS S3 (assuming credentials are set)\n",
        "s3 = boto3.client(\"s3\")\n",
        "bucket_name = \"your-bucket-name\"\n",
        "s3.put_object(Bucket=bucket_name, Key=\"aapl_statistical_memory.json\", Body=compressed_data)\n",
        "\n",
        "print(\"Statistical memory successfully uploaded to AWS S3!\")\n"
      ],
      "metadata": {
        "id": "Up2y4B6pMcUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}